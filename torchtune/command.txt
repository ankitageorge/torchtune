tune run --nnodes 1 --nproc_per_node 1 full_finetune_distributed \
            --config llama3/8B_full \
            batch_size=1 \
            gradient_accumulation_steps=4 \
            output_dir=/data/users/ankitageorge/tests \
            checkpointer._component_=torchtune.training.FullModelTorchTuneCheckpointer \
            checkpointer.checkpoint_dir=/data/users/ankitageorge/tests/tune_files \
            checkpointer.checkpoint_files=[/data/users/ankitageorge/tests/tune_files/small-ckpt-tune-llama3-05052024.pt] \
            checkpointer.output_dir=/data/users/ankitageorge/tests \
            checkpointer.model_type=LLAMA3 \
            tokenizer.path=/data/users/ankitageorge/tests/tune_files/tokenizer_llama3.model \
            tokenizer.prompt_template=null \
            metric_logger.filename=/data/users/ankitageorge/tests/tune_files/test.log \
            enable_async_checkpointing=True \
            clip_grad_norm=100 \
            optimizer_in_bwd=False \
            dtype=fp32 \
            enable_activation_checkpointing=False \
            enable_activation_offloading=False \
            dataset.train_on_input=False \
            seed=9 \
            epochs=2 \
            max_steps_per_epoch=2 \
            optimizer=torch.optim.AdamW \
            optimizer.lr=2e-5 \
            log_every_n_steps=1 \
            model._component_=torchtune.models.llama3.llama3 \
            model.vocab_size=128_256 \
            model.num_layers=2 \
            model.num_heads=8 \
            model.embed_dim=64 \
            model.max_seq_len=1024 \
            model.norm_eps=1e-5 \
            model.num_kv_heads=4 \
            dataset._component_=torchtune.datasets.alpaca_dataset\
            dataset.source='json'\
            dataset.data_files=/data/users/ankitageorge/tests/tune_files/alpaca_data.json\
            dataset.split='train'
